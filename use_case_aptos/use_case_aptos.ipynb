{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e5a4b6-0eb1-490d-9600-4ed87378ff89",
   "metadata": {},
   "source": [
    "# Use Case Complex Images : Aptos\n",
    "\n",
    "**Introduction:**\n",
    "\n",
    "Welcome to HES-Xplain, our interactive platform designed to facilitate explainable artificial intelligence (XAI) techniques. In this use case, we dive into sport image classification and showcase the utility of eXplainable AI (XAI) in this domain. We will focus on applying tools available on our platform to interpret and understand the decisions made by a Convolutional Neural Network (CNN) model.\n",
    "\n",
    "We also provide scripts, conveniently customizable, facilitate data loading, model training and evaluation, and performance metric plotting. Located in the scripts folder of this repository, they feature clear annotations to aid comprehension and instructions for effortless adaptation to your specific model needs.\n",
    "\n",
    "We will go through some state of the art XAI techniques for CNN models and some tools that wer designed by our team such as Rules Extraction or Fuzzy Logic. To do this we will use a pre-trained model, a diabetic retinopathy detection dataset, and HES-XPLAIN to interpret the model's predictions.\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "1. Understand the importance of interpretability in image classification.\n",
    "2. Introduce XAI techniques to interpret classification and understand a CNN model.\n",
    "3. Showcase the capabilities of HES-XPLAIN. \n",
    "4. Provide practical insights into applying such techniques through an interactive notebook.\n",
    "5. Foster a community of XAI enthusiasts and practitioners.\n",
    "\n",
    "**Outline:**\n",
    "\n",
    "1. Dataset and Problem Statement.\n",
    "2. Model Architecture.\n",
    "3. Modeling, training, and evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019df0db-adc3-4018-8dad-f9566020effe",
   "metadata": {},
   "source": [
    "## Google Colab Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1646e0ea-3e53-443c-975b-ee6a3864d949",
   "metadata": {},
   "source": [
    "This section prepares the notebook for use with Google Colaboratory. If applicable, change the following variable to **True**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9ae1d93-1683-4b93-b087-d18431568f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab compatibility\n",
    "use_colab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00658199-7a48-4b7f-8dda-cb84b41aa6a1",
   "metadata": {},
   "source": [
    "Prepare the code by downloading the code and installing required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08aea813-d798-48ce-a8a8-dfc4c6dede9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_colab:\n",
    "    # ensure the directory is empty\n",
    "    !rm -rf * .config\n",
    "\n",
    "    !# install codebase from GitHub\n",
    "    !git clone --no-checkout https://github.com/HES-XPLAIN/notebooks.git --depth=1 .\n",
    "    !git config core.sparseCheckout true\n",
    "    !git sparse-checkout set --cone\n",
    "    !git sparse-checkout add use_case_sport_classification\n",
    "    !git sparse-checkout reapply\n",
    "    !git checkout main\n",
    "\n",
    "    # adjust folder structure\n",
    "    !mv use_case_aptos/* .\n",
    "    !rm -rf use_case_aptos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72591da5-5726-4c24-a692-da45ae3fa7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_colab:\n",
    "    # Install dependencies\n",
    "    !pip install git+https://github.com/salesforce/OmniXAI.git@v1.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c133da28-8617-4f0c-baa7-edc157c7b67d",
   "metadata": {},
   "source": [
    "When asked to, restart the session by clicking on the \"Restart Session\" button, then continue and execute the next cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff2d8a0-a257-44be-8087-f791c927595a",
   "metadata": {},
   "source": [
    "## Workspace Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f1bfed-6cfd-4469-a21e-6c199c5b58a3",
   "metadata": {},
   "source": [
    "This section download the required code and models from our GitHub and huggingface.co repositories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951e7d2a-18a1-4341-9a99-1e759244b3c0",
   "metadata": {},
   "source": [
    "Download the model weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95792fb7-84bf-4b4d-bbfc-59d469ec62e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1147  100  1147    0     0   7750      0 --:--:-- --:--:-- --:--:--  7697\n",
      "100  512M  100  512M    0     0   104M      0  0:00:04  0:00:04 --:--:--  108M\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p models_weight\n",
    "!curl -OL https://huggingface.co/HES-XPLAIN/VGGAptos/resolve/main/VGGAptos_1908_v2.pth -o VGGAptos.pth\n",
    "!curl -OL https://huggingface.co/HES-XPLAIN/VGGAptos/resolve/main/VGG_augmented_1908_v2.pth -o VGGAptos_augmented.pth\n",
    "!mv *.pth models_weight/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bd8e02-80ce-4890-aeb4-12cd2b61cb37",
   "metadata": {},
   "source": [
    "---\n",
    "## Dataset and Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804f03d0-d309-4a24-aa7f-05edb2ebf691",
   "metadata": {},
   "source": [
    "The APTOS 2019 retina classification dataset utilized in this use case comprises approximately 3,662 images, encompassing a range of severity levels for diabetic retinopathy. This dataset, accessible on [Kaggle](https://www.kaggle.com/competitions/aptos2019-blindness-detection/), includes a total of 3,662 images, which are categorized into five severity levels of diabetic retinopathy (DR).\n",
    "The five classification categories are : \n",
    "- No DR\n",
    "- Mild\n",
    "- Moderate\n",
    "- Server\n",
    "- Proliferative DR\n",
    "\n",
    "**Problem Statement:** The goal is to develop a reliable image classifier capable of accurately diagnosing the severity of diabetic retinopathy from retina images. The dataset's focus on varying stages of this condition makes it ideal for applying and demonstrating XAI techniques. By employing deep learning models and XAI techniques, the objective is not only to achieve superior classification accuracy but also to understand the key regions within the retina images that influence the classification decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37910eef-7c6e-43be-b27c-3145ab7192ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global imports\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from omnixai_community.data.image import Image as omniImage\n",
    "from omnixai_community.explainers.vision import VisionExplainer\n",
    "from omnixai_community.explainers.vision.specific.gradcam.pytorch.gradcam import GradCAM, GradCAMPlus\n",
    "from omnixai_community.preprocessing.image import Resize\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.model import VGGAptos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae722ce6-9e7a-4cc5-b374-18a4c6a7369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d18de8-07a8-4111-a504-bbdb070e2c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global variables\n",
    "\n",
    "home_path = \"./\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "idx2class = {\n",
    "    0: 'No DR',\n",
    "    1: 'Mild',\n",
    "    2: 'Moderate',\n",
    "    3: 'Severe',\n",
    "    4: 'Proliferative DR'\n",
    "}\n",
    "\n",
    "class2idx = {\n",
    "    'No DR': 0,\n",
    "    'Mild': 1,\n",
    "    'Moderate': 2,\n",
    "    'Severe': 3,\n",
    "    'Proliferative DR': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a966b20-b522-4de7-b5d6-071d138d4c51",
   "metadata": {},
   "source": [
    "### Load the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b770ff7f-79b7-42be-a209-a81f5edcec95",
   "metadata": {},
   "source": [
    "In this use case, we leverage the power of transfer learning and fine-tuning by utilizing a pre-trained model. Specifically, we will be using a fine-tuned VGG model that have already learned valuable features from a large-scale dataset. To perform inference with this model, we will need to include the model class definition script.\n",
    "\n",
    "**Class Definition Script:**\n",
    "\n",
    "You can find the class definition script (`model.py`) in the `scripts` directory of this notebook that define a VGG model. You can create a similar class with the model of your needs,  just make sure to adapt it to the number of classes of your dataset, here it is 5 classes. \n",
    "\n",
    "The script contains the necessary custom class definitions and functions required for the sport image classification task. Make sure to include and import this script in your code to ensure proper model instantiation and inference.\n",
    "\n",
    "To load the fine-tuned weights into the model, you can use the following code snippet:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from scripts.model import YourCustomModelClass\n",
    "\n",
    "# Create an instance of your PyTorch model\n",
    "model = YourCustomModelClass()\n",
    "\n",
    "# Load the fine-tuned weights into the model\n",
    "model.load_state_dict(torch.load(\"path_to_your_fine_tuned_weights.pth\"))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Perform inference using the model\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f11fbf-5688-4b52-9d09-2efb7cd8253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.model import VGGAptos\n",
    "\n",
    "model = VGGAptos(mode='evaluation')\n",
    "model.load_state_dict(torch.load('./models_weight/VGGAptos_1908_v2.pth',  map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e60fb1-9e91-481b-b250-94474562898b",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1436c3-7b00-4c2b-b283-02e161ccda47",
   "metadata": {},
   "source": [
    "For now, we will utilize the VGG16 architecture, which has demonstrated excellent performance in image classification tasks. VGG16 models are known for their simplicity and effectiveness, making them widely used in various computer vision applications.\n",
    "\n",
    "VGG16 consists of several convolutional layers followed by fully connected layers. The architecture is characterized by its deep stack of convolutional layers, which allows it to learn complex features from input images.\n",
    "\n",
    "In our case, we will use the VGG16 model with the following specifications:\n",
    "\n",
    "- Model: VGG16\n",
    "- Pretrained: Yes\n",
    "- Number of classes: 100\n",
    "\n",
    "VGG16 is a well-established model in the computer vision community and has been pretrained on large-scale image datasets like ImageNet. By leveraging transfer learning and fine-tuning, we can utilize the pre-trained weights of VGG16 to enhance our model's performance on the sport image classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09397c9d-c368-4a2c-82ba-4aa620582970",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39934fe2-7656-40c3-8f13-c94cc97528ab",
   "metadata": {},
   "source": [
    "### Traininig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b3c58a-8766-45fe-8f3d-24be4a69af38",
   "metadata": {},
   "source": [
    "As previously described, we've loaded a pretrained model that underwent fine-tuning. However, if desired, you have the option to further fine-tune it using either our train scripts or the train functions provided, both defined in ./scripts/train.py. As we can't direclty provide an access to the dataset you will need to download the APTOS 2019 dataset and place it in a folder /data to match the script requirements before using it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c3b110-03d2-4e34-a82e-1c3fe416f0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "\n",
    "# using the script \n",
    "#!python scripts/train.py --epochs 100 --lr 1e-4 --name CustomName\n",
    "\n",
    "# or using the function \n",
    "# from scripts.train\n",
    "# and use it as you needs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37a9ff5-71eb-432b-b957-9129b38b1bab",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaa4c2d-2e5f-4b74-ad5a-18625c5a0530",
   "metadata": {},
   "source": [
    "To evaluate the performance of the model on the evaluation dataset, let's have a look at the pre-computed confusion matrix and see what is interesing. Once again you can compute it yourself alone or using the functions given in the scripts/helpers.py file. Here we plot a pre-computed matrix since we can not provide the test set directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0290a1-af12-4576-b958-f9531a112a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Open and display the image\n",
    "img = Image.open('plots/confusion_matrixVGGAptos_1908_v2.png')\n",
    "img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d20b757-7d2b-4ecf-a020-33fd3ed03d45",
   "metadata": {},
   "source": [
    "#### Understanding the Confusion Matrix: A Case Study\n",
    "\n",
    "In our confusion matrix, we notice that the \"No DR\" category is classified very accurately, with more than 99% of samples correctly identified. This high accuracy is likely because \"No DR\" is a clear and dominant category in our dataset. \n",
    "\n",
    "However, we observe that the other categories, like \"Moderate DR,\" \"Mild DR,\" \"Severe DR,\" and \"Proliferative DR,\" show varied levels of accuracy. For example:\n",
    "\n",
    "- \"Moderate DR\" has a reasonable accuracy of about 0.84.\n",
    "- \"Severe\" shows a very bad accuracy of 0.26.\n",
    "\n",
    "What’s particularly surprising is that the model seems to mistakenly classify many samples as \"Moderate DR,\" even when they belong to \"Mild,\" \"Severe,\" or \"Proliferative DR.\" Specifically, 36% of \"Mild\" samples and 45% of \"Severe\" samples are misclassified as \"Moderate.\" This is a notable issue.\n",
    "\n",
    "Looking at the number of samples in the original dataset (see the plot below) we find that the \"Moderate\" category is overrepresented compared to the other categories, except \"No DR.\" This overrepresentation might explain why the model tends to favor the \"Moderate DR\" classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e69b2-93ed-4958-ab3d-0f5df3a2782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and display the image\n",
    "img = Image.open('plots/bar_plot_classname.png')\n",
    "img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c493f5df-248b-4047-8824-ba175e9f180a",
   "metadata": {},
   "source": [
    "To improve our model, we can retrain it with adjustments for these dataset imbalances in order to remove this sampling bias. \n",
    "To do this we randomly selected No DR to select only 1k images and using random image transformations we increase the number of samples per catergory to reach 1k as shown below. \n",
    "\n",
    "Additionally, we can use Explainable AI (XAI) techniques to better understand where the model is making mistakes and why. This approach will help us enhance the model's performance and make it more accurate across all categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb914cd-6436-44a1-a9a2-c49200d7c387",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('plots/bar_plot_classname_balanced.png')\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44ca54e-eaf3-4f3f-889b-a5a41b608e7d",
   "metadata": {},
   "source": [
    "### Balancing the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea1ad0b",
   "metadata": {},
   "source": [
    "Now we can use this balanced dataset to fine tune our VGG model. And hopefully we will get rid of most effect due to imbalance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a331c4c5-8911-43cd-9293-eb1412a9aab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_balanced = VGGAptos(mode='evaluation')\n",
    "model_balanced.load_state_dict(torch.load('./models_weight/VGG_augmented_1908_v2.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65ced7a-6a04-49e8-bbc5-0cc2ffda4ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and display the image\n",
    "img = Image.open('plots/confusion_matrixVGG_augmented_1908_v2.png')\n",
    "img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ab8894-cf36-4733-bf23-38a0616b53a1",
   "metadata": {},
   "source": [
    "#### Model Evaluation and Next Steps\n",
    "\n",
    "After retraining our model on a balanced dataset, we observed some interesting results:\n",
    "\n",
    "- **F1 Score and Accuracy**: While the overall F1 score and accuracy haven't improved significantly and eveng get slightly worse, the confusion matrix shows a noticeable improvement. Now, all categories are classified with an accuracy of more than 0.72.\n",
    "- **Category-Specific Performance**: There is a slight 4% drop in the accuracy for the \"No DR\" category. However, this decrease is offset by improvements in the accuracy of other categories.\n",
    "- **Current Model Status**: Our model, trained on a balanced dataset, now provides more consistent and coherent classifications across different categories.\n",
    "\n",
    "Despite these improvements, there are still instances of incorrect classifications. To further enhance our model and understand its behavior better, the next step is to use Explainable AI (XAI) techniques. XAI will help us:\n",
    "\n",
    "1. **Make the Model More Understandable**: By using XAI, we can gain insights into how the model makes decisions.\n",
    "2. **Identify Potential Biases**: XAI can also help us spot any remaining biases or issues in the model.\n",
    "\n",
    "This approach will guide us in refining the model further and ensuring its predictions are as accurate and fair as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f030c8-f695-4f28-a7f8-9cc4f2b94e97",
   "metadata": {},
   "source": [
    "---\n",
    "Looking into the confusion matrix we can see that the worse classified category is \"proliferative DR\" which could be problematic since it is the worse condition category. We can see that 13% of the Profliferative DR are predicted as Moderate which is surprising because they are not neighbours category, let's try to find out what our model is doing wrong here. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075a03f8-5368-4b6c-bb9d-dd62ebd5e2f2",
   "metadata": {},
   "source": [
    "## Class Activation Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854be346",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "preprocess = lambda ims: torch.stack([transform(im.to_pil()) for im in ims]).to(device)\n",
    "postprocess = lambda logits: torch.nn.functional.softmax(logits, dim=1)   \n",
    "\n",
    "explainer = GradCAM(\n",
    "    model=model_balanced,\n",
    "    target_layer=model_balanced.features[-4],\n",
    "    preprocess_function=preprocess\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(8, 6, figsize=(30, 40))\n",
    "\n",
    "true_moderate_paths = select_random_images('./data/moderate', num_images=8)\n",
    "proliferative_dr_paths = select_random_images('./data/proliferative', num_images=8)\n",
    "false_moderate_paths = select_random_images('./data/pro_pred_as_mod', num_images=8)\n",
    "\n",
    "for i, img_path in enumerate(true_moderate_paths):\n",
    "    plot_image_and_score(explainer, axes[i, 0], axes[i, 1], img_path, idx2class, true_label=\"True Moderate\")\n",
    "\n",
    "for i, img_path in enumerate(proliferative_dr_paths):\n",
    "    plot_image_and_score(explainer, axes[i, 2], axes[i, 3], img_path, idx2class)\n",
    "\n",
    "\n",
    "for i, img_path in enumerate(false_moderate_paths):\n",
    "    plot_image_and_score(explainer, axes[i, 4], axes[i, 5], img_path, idx2class)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"gradcam_comparison.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f68b4-c6b0-47ba-83a2-d1515770397d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "outputs": [],
   "source": [
    "from omnixai_community.explainers.vision import VisionExplainer\n",
    "\n",
    "preprocess = lambda ims: torch.stack([transform(im.to_pil()) for im in ims]).to(device)\n",
    "postprocess = lambda logits: torch.nn.functional.softmax(logits, dim=1)   \n",
    "\n",
    "model_balanced = model_balanced.to(device)\n",
    "img = omniImage(Image.open('/mnt/remote/workspaces/arthur.babey/project/notebooks/use_case_aptos/data/moderate/0fd16b64697e.png'))\n",
    "explainer = VisionExplainer(\n",
    "    explainers=[\"gradcam\", \"lime\", \"ig\", \"ce\", \"scorecam\", \"smoothgrad\", \"guidedbp\", \"layercam\"],\n",
    "    mode=\"classification\",\n",
    "    model=model_balanced,\n",
    "    preprocess=preprocess,\n",
    "    postprocess=postprocess,\n",
    "    params={\"gradcam\": {\"target_layer\": model_balanced.features[-4]},\n",
    "            \"ce\": {\"binary_search_steps\": 2, \"num_iterations\": 100},\n",
    "            \"scorecam\": {\"target_layer\": model_balanced.features[-4]},\n",
    "            \"layercam\": {\"target_layer\": model_balanced.features[-7]},\n",
    "    }\n",
    ")\n",
    "\n",
    "local_explanations = explainer.explain(img)\n",
    "global_explanations = explainer.explain_global(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fdda79-5d63-46e3-8677-dbef73d998c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=0\n",
    "for name, explanations in local_explanations.items():\n",
    "    print(f\"{name}:\")\n",
    "    explanations.ipython_plot(index, class_names=idx2class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b308f588-465c-4e98-8090-1670641c95d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912668b-7436-487f-acbd-10c87265e4c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a83fff-8fe9-439b-99c1-7b1e0cbf8b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f62d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxplain.explainers.vision.specific.rulesextract import (  # noqa: F401\n",
    "    RulesExtractImage,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713eaee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca426b4-63cc-437e-98df-94d33a540664",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rules2",
   "language": "python",
   "name": "rules2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
