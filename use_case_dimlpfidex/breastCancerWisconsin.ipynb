{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6032ef9f-92e2-44dc-a11b-b224342bee29",
   "metadata": {},
   "source": [
    "# Exploring Dimlp and Fidex rule generation for breast cancer classification\n",
    "\n",
    "**Introduction:**\n",
    "\n",
    "By the end of this notebook, you'll have a solid understanding of how to use Dimlp to train the model and the Fidex algorithms to extract rules.\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "    1. Understand the importance of interpretability in medical problems.\n",
    "    2. Introduce Dimlp and Fidex as powerful XAI techniques.\n",
    "    3. Understand how to use Dimlp and Fidex.\n",
    "    4. Showcase the capabilities of HES-Xplain in implementing Dimlp and Fidex algorithms.\n",
    "    5. Provide practical insights into applying Dimlp and Fidex to breast cancer classifiers through an interactive notebook.\n",
    "    6. Foster a community of XAI enthusiasts and practitioners.\n",
    "\n",
    "**Outline:**\n",
    "\n",
    "    1. Dataset and Problem Statement.\n",
    "    2. Load the dataset.\n",
    "    3. Model training.\n",
    "    4. Local rules generation - Fidex.\n",
    "    5. Global ruleSet generation - FidexGlo.\n",
    "    6. Conclusion.\n",
    "    7. References."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e813050-5104-4b92-953b-341c3353ff2f",
   "metadata": {},
   "source": [
    "---\n",
    "## Google Colab Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb0be64-973d-4f62-895e-ce3ca8a75faa",
   "metadata": {},
   "source": [
    "This section prepares the notebook for use with Google Colaboratory. If applicable, change the following variable to True:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c525f220-236f-45a8-890d-a211643844b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab compatibility\n",
    "use_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6182cfe3-b6f5-4a8d-92a4-0d3c9115fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_colab:\n",
    "    # ensure the directory is empty\n",
    "    !rm -rf * .config\n",
    "\n",
    "    !# install codebase from GitHub\n",
    "    !git clone --no-checkout https://github.com/HES-XPLAIN/notebooks.git --depth=1 .\n",
    "    !git config core.sparseCheckout true\n",
    "    !git sparse-checkout set --cone\n",
    "    !git sparse-checkout add use_case_dimlpfidex\n",
    "    !git sparse-checkout reapply\n",
    "    !git checkout main\n",
    "\n",
    "    # adjust folder structure\n",
    "    !mv use_case_dimlpfidex/* .\n",
    "    !rm -rf use_case_dimlpfidex/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f882f8-9ee5-44a3-9091-bc5cd14c896e",
   "metadata": {},
   "source": [
    "# Dataset and Problem Statement\n",
    "\n",
    "The Breast Cancer Dataset used in this use case consists of 569 data samples representing patients who have tumors. Each patient has 30 attributes, computed out of an image, used to determine if the tumor is malignant (first class) or benign (second class). This dataset is available on [Kaggle](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data) and can be imported with sklearn. It is composed of 455 training samples and 114 testing samples. \n",
    "\n",
    "The 30 attributes are composed of 10 real-valued features computed for each cell nucleus :\n",
    "\n",
    "- **Radius**: Mean of distances from the center to points on the perimeter.\n",
    "- **Texture**: Standard deviation of gray-scale values.\n",
    "- **Perimeter**\n",
    "- **Area**\n",
    "- **Smoothness**: Local variation in radius lengths.\n",
    "- **Compactness**: $\\frac{\\text{perimeter}^2}{\\text{area}} - 1.0$\n",
    "- **Concavity**: Severity of concave portions of the contour.\n",
    "- **Concave Points**: Number of concave portions of the contour.\n",
    "- **Symmetry**\n",
    "- **Fractal Dimension**: \"Coastline approximation\" - 1.\n",
    "\n",
    "For each tumor image, the mean, standard error, and worst errors were computed and appear as different attributes.\n",
    "\n",
    "**Problem Statement:** Our objective is to build a robust classifier capable of accurately classifying breast cancer samples among the 2 classes. By leveraging deep learning techniques and Fidex algorithms, we aim to not only achieve high classification performance but also gain insights into the attributes that contribute to the classification decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfedb29-a2b0-4a64-9958-da3f1463a78d",
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "\n",
    "We'll start by importing all libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c2697a5-c92e-40a6-86ce-018b897baea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.utils import shuffle\n",
    "from dimlpfidex.dimlp import dimlpBT\n",
    "from IPython.display import Image, display\n",
    "from trainings.normalization import normalization\n",
    "from trainings.computeRocCurve import computeRocCurve\n",
    "from dimlpfidex.fidex import fidex, fidexGloRules, fidexGloStats, fidexGlo\n",
    "\n",
    "class Unbuffered(object):\n",
    "   def __init__(self, stream):\n",
    "       self.stream = stream\n",
    "   def write(self, data):\n",
    "       self.stream.write(data)\n",
    "       self.stream.flush()\n",
    "   def writelines(self, datas):\n",
    "       self.stream.writelines(datas)\n",
    "       self.stream.flush()\n",
    "   def __getattr__(self, attr):\n",
    "       return getattr(self.stream, attr)\n",
    "\n",
    "\n",
    "sys.stdout = Unbuffered(sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade571b-4876-45b5-9d09-a76791165a75",
   "metadata": {},
   "source": [
    "We download the dataset with sklearn, then shuffle it randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd7b7b29-d07d-4750-820f-2270937d8ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datas, classes = shuffle(load_breast_cancer().data, load_breast_cancer().target)\n",
    "\n",
    "dataset, classes = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "classesLabels = list(load_breast_cancer().target_names)\n",
    "\n",
    "# classes one-hot encoding \n",
    "classes = pd.get_dummies(classes, dtype=\"uint8\")\n",
    "\n",
    "# rename classes\n",
    "classes = classes.rename(\n",
    "    columns={\n",
    "        classes.columns[0]: classesLabels[0],\n",
    "        classes.columns[1]: classesLabels[1],\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d87dd5-2233-43e2-9f95-777a1f7bda99",
   "metadata": {},
   "source": [
    "We separate the dataset into a training set (75%) and test set (25%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a0ed78b-b077-4136-84a6-9f176f33c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbSamples = dataset.shape[0]\n",
    "nbAttributes = len(dataset.columns)\n",
    "nbClasses = len(classes.columns)\n",
    "\n",
    "cutoff = int(0.75 * nbSamples)\n",
    "\n",
    "trainData = dataset.iloc[:cutoff, :]\n",
    "testData = dataset.iloc[cutoff:, :]\n",
    "trainClasses = classes.iloc[:cutoff]\n",
    "testClasses = classes.iloc[cutoff:]\n",
    "\n",
    "assert (trainData.shape[0] + testData.shape[0]) == nbSamples\n",
    "assert (trainClasses.shape[0] + testClasses.shape[0]) == nbSamples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d123e58b-b85d-4b43-9e1d-fcc11b65eaa3",
   "metadata": {},
   "source": [
    "We display the number of training and testing data and check that there is the same number of data as classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ce94b0e-e7ab-4d8f-9082-cb57da8b17cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 426 train datas and 426 train classes\n",
      "There are 143 test datas and 143 test classes\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {trainData.shape[0]} train datas and {trainClasses.shape[0]} train classes\")\n",
    "print(f\"There are {testData.shape[0]} test datas and {testClasses.shape[0]} test classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7724b51a-a76f-4e3e-9e27-b1b0a40a148d",
   "metadata": {},
   "source": [
    "Now, we save the all the data in text files in a specific format into BCWDataset directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a104b59-41da-4963-b799-70cb8544f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Folder '{folder_path}' created successfully.\")\n",
    "\n",
    "rootDir = \"data/BCWDataset/\"\n",
    "create_folder(rootDir)\n",
    "\n",
    "# defining filenames\n",
    "datasetFile = \"datas.txt\"\n",
    "trainDataFile = \"trainData.txt\"\n",
    "testDataFile = \"testData.txt\"\n",
    "trainClassFile = \"trainClass.txt\"\n",
    "testClassFile = \"testClass.txt\"\n",
    "\n",
    "dataset.to_csv(rootDir+datasetFile, header=False, index=False)\n",
    "3\n",
    "testData.to_csv(rootDir+testDataFile, header=False, index=False)\n",
    "testClasses.to_csv(rootDir+testClassFile, header=False, index=False)\n",
    "trainData.to_csv(rootDir+trainDataFile, header=False, index=False)\n",
    "trainClasses.to_csv(rootDir+trainClassFile, header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a05c5c-ad03-4e58-96d7-6bbe04f79a0c",
   "metadata": {},
   "source": [
    "Finally, we normalize the data so that the model learns better. We save the mean and std of each attribute in the file normalization_stats.txt. We use the normalization file located in the trainings module. To display the parameters, you can just call normalization with no parameters, or with -h or --help. Without further details, we normalize with this command :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "350d6c19-c2af-4a8e-893f-4b86fa9460e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters list:\n",
      " - root_folder                                                   data/BCWDataset/\n",
      " - nb_attributes                                                 30\n",
      " - data_files                                                    ['data/BCWDataset/trainData.txt', 'data/BCWDataset/testData.txt']\n",
      " - missing_values                                                NaN\n",
      " - normalization_indices                                         [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      " - output_normalization_file                                     data/BCWDataset/normalization_stats.txt\n",
      " - output_data_files                                             ['data/BCWDataset/trainData_normalized.txt', 'data/BCWDataset/testData_normalized.txt']\n",
      " - with_median                                                   False\n",
      " - fill_missing_values                                           True\n",
      "End of Parameters list. \n",
      "\n",
      "\n",
      "Full execution time = 0.306102 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalization(\"--help\")\n",
    "\n",
    "args = f\"\"\"\n",
    "        --data_files [{trainDataFile},{testDataFile}] \n",
    "        --nb_attributes {nbAttributes} \n",
    "        --missing_values NaN \n",
    "        --root_folder {rootDir}\n",
    "        \"\"\"\n",
    "normalization(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b34fc4-1e80-4eab-84ec-a0a4a28626fb",
   "metadata": {},
   "source": [
    "To show attribute names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6634d478-316f-464d-9199-a8ea39883b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'radius error', 'texture error', 'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "attributes = list(dataset.columns)\n",
    "print(attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2ed3d0-cb53-4371-9ec8-76500422c93d",
   "metadata": {},
   "source": [
    "And class names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d5ee674-09b8-47a2-a927-d2b3de0c3c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malignant', 'benign']\n"
     ]
    }
   ],
   "source": [
    "classes = list(classes.columns)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb8d44f-be2c-485a-99dc-5e90ea7e74a1",
   "metadata": {},
   "source": [
    "We store attribute and class names in a file named `attributes.txt` to identify features within rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba892ed9-0ae5-4711-a998-07bc668764f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributesFileName = \"attributes.txt\" \n",
    "attributesFileContent = attributes + classesLabels\n",
    "attributesFileContent = [x.replace(\" \", \"_\") for x in attributesFileContent]\n",
    "\n",
    "with open(rootDir+attributesFileName, \"w\") as f:\n",
    "    for item in attributesFileContent:\n",
    "        f.write(item+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c01997-d95c-4c99-8ea4-3dfa31d7ba2f",
   "metadata": {},
   "source": [
    "# Model training\n",
    "Here is an example of how to train a DimlpBT model with our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25db56b3",
   "metadata": {},
   "source": [
    "First, let's display all the DimlpBT parameters available:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69eb22f",
   "metadata": {},
   "source": [
    "To configure our training, we will use this configuration file, located at `data/BCWTemplates/config_dimlpBT.json`:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"root_folder\": \"data/BCWDataset\",\n",
    "    \"train_data_file\": \"trainData_normalized.txt\",\n",
    "    \"train_class_file\": \"trainClass.txt\",\n",
    "    \"test_data_file\": \"testData_normalized.txt\",\n",
    "    \"test_class_file\": \"testClass.txt\",\n",
    "    \"attributes_file\": \"attributes.txt\",\n",
    "    \"train_pred_outfile\": \"predTrain.out\",\n",
    "    \"test_pred_outfile\": \"predTest.out\",\n",
    "    \"console_file\": \"resultDimlpBT.txt\",\n",
    "    \"with_rule_extraction\": true,\n",
    "    \"global_rules_outfile\": \"dimlpBTRules.rls\",\n",
    "    \"weights_outfile\": \"weights.wts\",\n",
    "    \"stats_file\": \"stats.txt\",\n",
    "    \"nb_attributes\": 30,\n",
    "    \"nb_classes\": 2,\n",
    "    \"hidden_layers\": [5]\n",
    "}\n",
    "```\n",
    "\n",
    "> A detailed list of parameters is available [here](https://hes-xplain.github.io/documentation/algorithms/dimlp/dimlpbt/). If you want to create your own configuration file, we recommend using our [configuration file creator](https://hes-xplain.github.io/documentation/gui/)\n",
    "\n",
    "\n",
    "\n",
    "We can start training our model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e804d87b-b2dc-4946-9e99-caf1fa3fa066",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dimlpBT(\"--json_config_file data/BCWTemplates/config_dimlpBT.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c12044-2b09-4b7c-82b4-e6285ef55dfc",
   "metadata": {},
   "source": [
    "You can check all outputs inside the `data/BCWDataset` folder.\n",
    "\n",
    "Inside `stats.txt` you can see the train and test accuracy as well as the sum squared errors. \n",
    "\n",
    "There are also the train predictions, test predictions, the weights, and the dimlp rules files.\n",
    "\n",
    "In the file `dimlpBTRules.txt`, you can see the rule set, some statistics on the rules, and some statistics on the ruleset. Each rule is composed of antecedents and target class as well as the number of covering of the rule, which is the number of examples that verify (or \"activate\") the rule (even if the class is not correct).\n",
    "\n",
    "In the `Training set` and `Testing set` sections, each rule is composed, left to right, of:\n",
    "\n",
    "The number of covered samples, the number of correct covered samples, the number of false covered samples, and the accuracy of the rule.\n",
    "\n",
    "Between the `Training set` and `Testing set` sections, There are statistics of the ruleset:\n",
    "\n",
    "- The number of rules in the set.\n",
    "- The mean total and the mean number of antecedents per rule.\n",
    "- The covering of the rule.\n",
    "- The rules' accuracy.\n",
    "- The fidelity to the model is the percentage of covered samples that are correct concerning the model's decision.\n",
    "- The model accuracy if we keep only the samples for which the model and the rules decision agree.\n",
    "- The default rule activation rate, which is the percentage of samples for which no rule is activated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0865c6d-1648-4a61-8962-e81690b8d491",
   "metadata": {},
   "source": [
    "# Local rules generation - Fidex\n",
    "\n",
    "Fidex is an algorithm used in classification problems that allows us to obtain a rule explaining the decision class of the model for a given test sample. It enables us to grasp the model's decision-making process and to better understand the importance of each parameter in discerning the nature of the tumor, distinguishing between benign and malignant cases.\n",
    "\n",
    "Now we can generate some local rules to explain the models' results. We can start with launching Fidex on one test sample. This will generate a rule explaining the sample locally. It is local because the algorithm searches a rule only for one sample.\n",
    "Fidex is located in the fidex module. Let's take a look at the parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "368222c7-c0f8-4af0-a6a7-7adf290b7192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Warning! The files are located with respect to the root folder dimlpfidex.\n",
      "The arguments can be specified in the command or in a json configuration file with --json_config_file your_config_file.json.\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Required parameters:\n",
      "\n",
      "--train_data_file <str>       Path to the file containing the train portion of the dataset\n",
      "--train_class_file <str>      Path to the file containing the train true classes of the dataset, not mandatory if classes are specified in train data file\n",
      "--train_pred_file <str>       Path to the file containing predictions on the train portion of the dataset\n",
      "--test_data_file <str>        Path to the file containing the test sample(s) data, prediction (if no --test_pred_file) and true class(if no --test_class_file)\n",
      "--test_pred_file <str>        Path to the file containing predictions on the test portion of the dataset\n",
      "--weights_file <str>          Path to the file containing the trained weights of the model (not mandatory if a rules file is given with --rules_file)\n",
      "--rules_file <str>            Path to the file containing the trained rules to be converted to hyperlocus (not mandatory if a weights file is given with --weights_file)\n",
      "--rules_outfile <str>         Path to the file where the output rule(s) will be stored. If a .json extension is given, rules are saved in JSON format\n",
      "--nb_attributes <int [1,inf[> Number of attributes in the dataset\n",
      "--nb_classes <int [2,inf[>    Number of classes in the dataset\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Optional parameters: \n",
      "\n",
      "-h --help                     Show this help message and exit\n",
      "--json_config_file <str>      Path to the JSON file that configures all parameters. If used, this must be the sole argument and must specify the file's relative path\n",
      "--root_folder <str>           Path to the folder, based on main default folder dimlpfidex, containing all used files and where generated files will be saved. If a file name is specified with another option, its path will be relative to this root folder\n",
      "--test_class_file <str>       Path to the file containing the test true classes of the dataset. If at least --test_pred_file is specified, --test_data_file needs to have only test datas and eventually classes on same line (don't add --test_class_file in this case)\n",
      "--attributes_file <str>       Path to the file containing the labels of attributes and classes\n",
      "--stats_file <str>            Path to the file where statistics concerning the algorithm execution will be stored\n",
      "--console_file <str>          Path to the file where the terminal output will be redirected. If not specified, all output will be shown on your terminal\n",
      "--max_iterations <int [1,inf[>\n",
      "                              Maximum number of iterations, also the maximum possible number of antecedents in a rule, it should be 25 when working with images (default: 10)\n",
      "--min_covering <int [1,inf[>  Minimum number of samples covered by the generated rules (default: 2)\n",
      "--covering_strategy <bool>    Whether to use the covering strategy : if no rule is found with min_covering, find best rule with best covering using dichotomic search. Decreases min_fidelity if needed (default: True)\n",
      "--max_failed_attempts <int [0,inf[>\n",
      "                              Maximum number of failed attempts to find a Fidex rule when the covering is 1 and the covering strategy is used (default: 30)\n",
      "--min_fidelity <float [0,1]>  Minimal rule fidelity accepted when generating a rule (default: 1.0)\n",
      "--lowest_min_fidelity <float [0,1]>\n",
      "                              Minimal min_fidelity to which we agree to go down during the covering_strategy (default: 0.75)\n",
      "--dropout_dim <float [0,1]>   Probability of dropping a dimension during rule extraction (default: 0.0)\n",
      "--dropout_hyp <float [0,1]>   Probability of dropping a hyperplane during rule extraction (default: 0.0)\n",
      "--decision_threshold <float [0,1]>\n",
      "                              The decision threshold used for predictions, you need to specify the index of the positive class if you want to use it\n",
      "--positive_class_index <int [0,nb_classes-1]>\n",
      "                              Index of the positive class for the usage of a decision threshold, index starts at 0\n",
      "--nb_quant_levels <int [3,inf[>\n",
      "                              Number of stairs in the staircase activation function (default: 50)\n",
      "--normalization_file <str>    Path to the file containing the mean and standard deviation of some attributes. Used to denormalize the rules if specified\n",
      "--mus <list<float ]-inf,inf[>>\n",
      "                              Mean or median of each attribute index to be denormalized in the rules\n",
      "--sigmas <list<float ]-inf,inf[>>\n",
      "                              Standard deviation of each attribute index to be denormalized in the rules\n",
      "--normalization_indices <list<int [0,nb_attributes-1]>>\n",
      "                              Attribute indices to be denormalized in the rules, only used when no normalization_file is given, index starts at 0 (default: [0,...,nb_attributes-1])\n",
      "--seed <int [0,inf[>          Seed for random number generation, 0=random. Anything else than 0 is an arbitrary seed that can be reused to obtain the same randomly generated sequence and therefore getting same results (default: 0)\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Execution example :\n",
      "\n",
      "fidex.fidex(\"--train_data_file datanormTrain.txt --train_pred_file predTrain.out --train_class_file dataclass2Train.txt --test_data_file testSampleDataCombine.txt --nb_attributes 16 --nb_classes 2 --weights_file weights.wts --rules_outfile rules.rls --stats_file stats.txt --root_folder dimlp/datafiles\")\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = fidex(\"--help\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4603d85-d0eb-4fb1-bd50-e66cac47b936",
   "metadata": {},
   "source": [
    "We see that we need the number of attributes and classes, the train and test data and class files, as well as train and test predictions, weights file name, and rule output file name where the rules will be saved.<br>\n",
    "We specify as well the saved folder and the attributes.<br>\n",
    "We also need to specify the normalization file obtained from training, to denormalize the values in the generated rule, otherwise the values will be normalized and impossible to interpret.<br>\n",
    "\n",
    "To see what happens, we launch it with just one sample, and we save beforehand the test data sample in a file with its class and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "add3ff77-f54f-4288-b7fa-3c780a617653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters list:\n",
      " - train_data_file                                             data/BCWDataset/trainData_normalized.txt\n",
      " - train_pred_file                                                        data/BCWDataset/predTrain.out\n",
      " - train_class_file                                                      data/BCWDataset/trainClass.txt\n",
      " - test_data_file                                                    data/BCWDataset/testDataSample.txt\n",
      " - rules_outfile                                                               data/BCWDataset/rule.rls\n",
      " - root_folder                                                                          data/BCWDataset\n",
      " - attributes_file                                                       data/BCWDataset/attributes.txt\n",
      " - weights_file                                                             data/BCWDataset/weights.wts\n",
      " - normalization_file                                           data/BCWDataset/normalization_stats.txt\n",
      " - nb_attributes                                                                                     30\n",
      " - nb_classes                                                                                         2\n",
      " - nb_quant_levels                                                                                   50\n",
      " - max_iterations                                                                                    10\n",
      " - min_covering                                                                                       2\n",
      " - max_failed_attempts                                                                               30\n",
      " - positive_class_index                                                                              -1\n",
      " - seed                                                                                               0\n",
      " - decision_threshold                                                                         -1.000000\n",
      " - hi_knot                                                                                     5.000000\n",
      " - dropout_hyp                                                                                 0.000000\n",
      " - dropout_dim                                                                                 0.000000\n",
      " - min_fidelity                                                                                1.000000\n",
      " - lowest_min_fidelity                                                                         0.750000\n",
      " - covering_strategy                                                                                  1\n",
      "End of Parameters list.\n",
      "\n",
      "Import files...\n",
      "\n",
      "Import time = 0.134617 sec\n",
      "Files imported\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "Creation of hyperspace...\n",
      "\n",
      "Parameters of hyperLocus :\n",
      "\n",
      "- Number of stairs 50\n",
      "- Interval : [-5,5]\n",
      "\n",
      "Import weight file...\n",
      "HyperLocus computed\n",
      "\n",
      "Hyperspace created\n",
      "\n",
      "Searching for discriminating hyperplans...\n",
      "Initial fidelity : 0.598592\n",
      "Final fidelity : 1\n",
      "Discriminating hyperplans generated.\n",
      "\n",
      "\n",
      "Extracted rule :\n",
      "mean_concave_points<0.052159 worst_texture>=39.681917 -> benign\n",
      "   Train Covering size : 2\n",
      "   Train Fidelity : 1\n",
      "   Train Accuracy : 1\n",
      "   Train Confidence : 0.890008\n",
      "\n",
      "Result found after 2 iterations.\n",
      "-------------------------------------------------\n",
      "\n",
      "Time without data import = 2.77765 sec\n",
      "\n",
      "Full execution time = 2.91341 sec\n"
     ]
    }
   ],
   "source": [
    "testPredsFile = \"predTest.out\"\n",
    "testSampleFile = \"testDataSample.txt\"\n",
    "testPreds = pd.read_csv(rootDir+testPredsFile, sep=\" \", header=None, index_col=None).iloc[:, :nbClasses]\n",
    "testDatasNormalized = pd.read_csv(rootDir+testDataFile, header=None, index_col=None)\n",
    "\n",
    "sampleSelected = 0\n",
    "assert(sampleSelected < nbSamples)\n",
    "\n",
    "# extract a sample to generate local rule\n",
    "sampleData = testData.iloc[sampleSelected, :nbAttributes].to_list()\n",
    "samplePred = testPreds.iloc[sampleSelected, :].to_list()\n",
    "sampleClasses = testData.iloc[sampleSelected, nbAttributes:].to_list()\n",
    "\n",
    "# write the sample, classes and predictions in the testSampleFile file (file writing format must be respected)\n",
    "with open(rootDir+testSampleFile, 'w') as f:\n",
    "    f.write(\" \".join(str(x) for x in sampleData) + '\\n')\n",
    "    f.write(\" \".join(str(x) for x in samplePred) + '\\n')\n",
    "    f.write(\" \".join(str(x) for x in sampleClasses) + '\\n')\n",
    "\n",
    "#res = fidex.fidex(\"--train_data_file trainData_normalized.txt --train_pred_file predTrain.out --train_class_file trainClass.txt --test_data_file testDataSample.txt --weights_file weights.wts --attributes_file attributes.txt --rules_outfile rule.rls --nb_attributes 30 --nb_classes 2 --normalization_file normalization_stats.txt --root_folder data/BCWDataset\")\n",
    "res = fidex(\"--json_config_file data/BCWTemplates/config_fidexOne.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dcf258-9284-4a41-817b-9158f321a2df",
   "metadata": {},
   "source": [
    "You can see the walkthrough of the algorithm and the rule extracted. The rule is also saved in the rule.rls file. With the rule, we see also the covering size of the rule on the training set, the fidelity, the accuracy, and the confidence of the rule. The confidence shows how much the rule is confident with his choices, with respect to the prediction values.<br>\n",
    "\n",
    "Now, we execute Fidex with all test samples. We send the console output in the fidexResult.txt file and save the global statistics in fidexStats.txt. **It should take about a minute**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2475dc1f-0815-476b-a695-9869598dcd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidex done\n"
     ]
    }
   ],
   "source": [
    "#res = fidex.fidex(\"--train_data_file trainData_normalized.txt --train_pred_file predTrain.out --train_class_file trainClass.txt --test_data_file testData_normalized.txt --test_class_file testClass.txt --test_pred_file predTest.out --weights_file weights.wts --attributes_file attributes.txt --rules_outfile fidexRules.rls --stats_file fidexStats.txt --console_file fidexResult.txt --nb_attributes 30 --nb_classes 2 --normalization_file normalization_stats.txt --root_folder data/BCWDataset\")\n",
    "res = fidex(\"--json_config_file data/BCWTemplates/config_fidex.json\")\n",
    "\n",
    "if (res == 0):\n",
    "    print(\"Fidex done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c4f8fe-ec3d-4231-866a-e4361dd323be",
   "metadata": {},
   "source": [
    "You can see the rules generated for each sample in the file fidexRules.rls. The global statistics on the test set appear in statsFidex.txt. There is the mean covering size per rule, the mean number of antecedents per rule, and the mean rule fidelity, accuracy, and confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2c1cad-7802-47e5-bc09-aa28967913cc",
   "metadata": {},
   "source": [
    "# Global ruleSet generation - FidexGlo\n",
    "We've seen how to compute a rule that explains the decision of the model for a specific sample. Now, we will generalize a ruleset that characterizes the whole train dataset. That means that for each training sample, there is a rule in the set of rules that explains the model's decision for this sample. We will use this global ruleset to explain the results obtained on new test samples. If there is a rule of the ruleset corresponding to the sample, we take this one and get a global explanation for the sample. If there is none, we call Fidex and only have a local explanation.<br>\n",
    "\n",
    "To get the ruleSet we execute fidexGloRules which is located in the fidex module. Here are the possible parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8286ec5a-306b-4048-84cb-7358c2647863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Warning! The files are located with respect to the root folder dimlpfidex.\n",
      "The arguments can be specified in the command or in a json configuration file with --json_config_file your_config_file.json.\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Required parameters:\n",
      "\n",
      "--train_data_file <str>       Path to the file containing the train portion of the dataset\n",
      "--train_class_file <str>      Path to the file containing the train true classes of the dataset, not mandatory if classes are specified in train data file\n",
      "--train_pred_file <str>       Path to the file containing predictions on the train portion of the dataset\n",
      "--weights_file <str>          Path to the file containing the trained weights of the model (not mandatory if a rules file is given with --rules_file)\n",
      "--rules_file <str>            Path to the file containing the trained rules to be converted to hyperlocus (not mandatory if a weights file is given with --weights_file)\n",
      "--global_rules_outfile <str>  Path to the file where the output rule(s) will be stored. If a .json extension is given, rules are saved in JSON format\n",
      "--heuristic <int [1,3]>       Heuristic 1: optimal fidexGlo, 2: fast fidexGlo 3: very fast fidexGlo. (Faster algorithms are less efficient)\n",
      "--nb_attributes <int [1,inf[> Number of attributes in the dataset\n",
      "--nb_classes <int [2,inf[>    Number of classes in the dataset\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Optional parameters: \n",
      "\n",
      "-h --help                     Show this help message and exit\n",
      "--json_config_file <str>      Path to the JSON file that configures all parameters. If used, this must be the sole argument and must specify the file's relative path\n",
      "--root_folder <str>           Path to the folder, based on main default folder dimlpfidex, containing all used files and where generated files will be saved. If a file name is specified with another option, its path will be relative to this root folder\n",
      "--attributes_file <str>       Path to the file containing the labels of attributes and classes\n",
      "--console_file <str>          Path to the file where the terminal output will be redirected. If not specified, all output will be shown on your terminal\n",
      "--max_iterations <int [1,inf[>\n",
      "                              Maximum number of iterations, also the maximum possible number of antecedents in a rule, it should be 25 when working with images (default: 10)\n",
      "--min_covering <int [1,inf[>  Minimum number of samples covered by the generated rules (default: 2)\n",
      "--covering_strategy <bool>    Whether to use the covering strategy : if no rule is found with min_covering, find best rule with best covering using dichotomic search. Decreases min_fidelity if needed (default: True)\n",
      "--max_failed_attempts <int [0,inf[>\n",
      "                              Maximum number of failed attempts to find a Fidex rule when the covering is 1 and the covering strategy is used (default: 30)\n",
      "--min_fidelity <float [0,1]>  Minimal rule fidelity accepted when generating a rule (default: 1.0)\n",
      "--lowest_min_fidelity <float [0,1]>\n",
      "                              Minimal min_fidelity to which we agree to go down during the covering_strategy (default: 0.75)\n",
      "--dropout_dim <float [0,1]>   Probability of dropping a dimension during rule extraction (default: 0.0)\n",
      "--dropout_hyp <float [0,1]>   Probability of dropping a hyperplane during rule extraction (default: 0.0)\n",
      "--decision_threshold <float [0,1]>\n",
      "                              The decision threshold used for predictions, you need to specify the index of the positive class if you want to use it\n",
      "--positive_class_index <int [0,nb_classes-1]>\n",
      "                              Index of the positive class for the usage of a decision threshold, index starts at 0\n",
      "--nb_quant_levels <int [3,inf[>\n",
      "                              Number of stairs in the staircase activation function (default: 50)\n",
      "--normalization_file <str>    Path to the file containing the mean and standard deviation of some attributes. Used to denormalize the rules if specified\n",
      "--mus <list<float ]-inf,inf[>>\n",
      "                              Mean or median of each attribute index to be denormalized in the rules\n",
      "--sigmas <list<float ]-inf,inf[>>\n",
      "                              Standard deviation of each attribute index to be denormalized in the rules\n",
      "--normalization_indices <list<int [0,nb_attributes-1]>>\n",
      "                              Attribute indices to be denormalized in the rules, only used when no normalization_file is given, index starts at 0 (default: [0,...,nb_attributes-1])\n",
      "--nb_threads <int [1,nb_cores]>\n",
      "                              Number of threads used for computing the algorithm, 1=sequential execution (default: 1)\n",
      "--seed <int [0,inf[>          Seed for random number generation, 0=random. Anything else than 0 is an arbitrary seed that can be reused to obtain the same randomly generated sequence and therefore getting same results (default: 0)\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Execution example :\n",
      "\n",
      "fidex.fidexGloRules(\"--train_data_file datanormTrain.txt --train_pred_file predTrain.out --train_class_file dataclass2Train.txt --weights_file weights.wts --nb_attributes 16 --nb_classes 2 --heuristic 1 --global_rules_outfile globalRules.rls --root_folder dimlp/datafiles\")\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = fidexGloRules(\"--help\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665be28c-c788-41c2-ae00-b7a7b3118c2b",
   "metadata": {},
   "source": [
    "We use nearly the same parameters as for Fidex but we only need train data. We need to choose a heuristic for fidexGloRules, we choose the optimal to get better results. We don't forget to add the normalization file.<br>\n",
    "**It should take about 3 minutes**. If you have several processors available, you should add the parameter nb_threads with the number of processors that you want to use, it can speed up the process a lot. If you want to accelerate the process even more, you can use some dropout, the algorithm will randomly skip some dimensions or some hyperplans. For example, you can put: -d 0.5 -h 0.5, to skip half dimensions and half hyperplans, which should be about 4 times faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26e0d897-ab14-4e8d-a0f9-da7fe35bfb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FidexGloRules done\n"
     ]
    }
   ],
   "source": [
    "#res = fidex.fidexGloRules(\"--train_data_file trainData_normalized.txt --train_pred_file predTrain.out --train_class_file trainClass.txt --weights_file weights.wts --attributes_file attributes.txt --nb_attributes 30 --nb_classes 2 --heuristic 1 --global_rules_outfile globalRules.rls --console_file fidexGloRulesResult.txt --normalization_file normalization_stats.txt --root_folder data/BCWDataset\")\n",
    "res = fidexGloRules(\"--json_config_file data/BCWTemplates/config_fidexGloRules.json\")\n",
    "if (res == 0):\n",
    "    print(\"FidexGloRules done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e7b780-5fc6-4d0a-a82e-1b86ed6853d4",
   "metadata": {},
   "source": [
    "You can see the console result in the file fidexGloRulesResult.txt and the ruleset in the file globalRules.rls.\n",
    "The algorithm is random, so each execution may generate different rules and a different number of them. It should generate about 25 rules. You can see at the top of the globalRules file the number of rules, the mean covering number per rule, and the mean number of antecedents. Here is an example of a rule that you may obtain:<br>\n",
    "\n",
    "Rule 1: WORST_CONCAVE_POINTS<0.111377 AREA_ERROR<37.691316 -> BENIGN <br>\n",
    "   Train Covering size : 217 <br>\n",
    "   Train Fidelity : 1 <br>\n",
    "   Train Accuracy : 0.990783 <br>\n",
    "   Train Confidence : 0.984811 <br>\n",
    "\n",
    "This rule is the first rule, which means that it's the rule with the maximum covering. Here, 217 train samples verify this rule. She is 100% fidel with the model and has about 99% train accuracy.\n",
    "This rule says that if you have worst concave points lower than 0.111377 and an area error lower than 37.691316, then your breast cancer is benign. And this rule is accurate, on train test, at 99% and has 98% of confidence.<br>\n",
    "\n",
    "Now, we can see some global statistics on the test set, and also some statistics directly on each rule. So we will see the test accuracy on this rule.<br>\n",
    "We execute fidexGloStats which is located in the fidex module. First, let's check the parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d6aaa70-4ffa-4205-99f8-840d67b260cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Warning! The files are located with respect to the root folder dimlpfidex.\n",
      "The arguments can be specified in the command or in a json configuration file with --json_config_file your_config_file.json.\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Required parameters:\n",
      "\n",
      "--test_data_file <str>        Path to the file containing the test portion of the dataset\n",
      "--test_class_file <str>       Path to the file containing the test true classes of the dataset, not mandatory if classes are specified in test data file\n",
      "--test_pred_file <str>        Path to the file containing predictions on the test portion of the dataset\n",
      "--global_rules_file <str>     Path to the file containing the global rules obtained with fidexGloRules algorithm.\n",
      "--nb_attributes <int [1,inf[> Number of attributes in the dataset\n",
      "--nb_classes <int [2,inf[>    Number of classes in the dataset\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Optional parameters: \n",
      "\n",
      "-h --help                     Show this help message and exit\n",
      "--json_config_file <str>      Path to the JSON file that configures all parameters. If used, this must be the sole argument and must specify the file's relative path\n",
      "--root_folder <str>           Path to the folder, based on main default folder dimlpfidex, containing all used files and where generated files will be saved. If a file name is specified with another option, its path will be relative to this root folder\n",
      "--global_rules_outfile <str>  Path to the file where the output global rules will be stored with stats on test set, if you want to compute those statistics. If a .json extension is given, rules are saved in JSON format\n",
      "--attributes_file <str>       Path to the file containing the labels of attributes and classes> Mandatory if rules file contains attribute names, if not, do not add it\n",
      "--stats_file <str>            Path to the file where statistics of the global ruleset will be stored\n",
      "--console_file <str>          Path to the file where the terminal output will be redirected. If not specified, all output will be shown on your terminal\n",
      "--positive_class_index <int [0,nb_classes-1]>\n",
      "                              Index of the positive class to compute true/false positive/negative rates, index starts at 0. If it is specified in the rules file, it has to be the same value.\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Execution example :\n",
      "\n",
      "fidex.fidexGloStats(\"--test_data_file datanormTest.txt --test_pred_file predTest.out --test_class_file dataclass2Test.txt --global_rules_file globalRules.rls --nb_attributes 16 --nb_classes 2 --stats_file stats.txt --root_folder dimlp/datafiles\")\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = fidexGloStats(\"--help\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b7ebed-01b9-4016-addb-d99839f7be74",
   "metadata": {},
   "source": [
    "We need the test files(data, prediction, class), the rules file, and the number of attributes and classes. We need to specify the attribute file as well. We choose to save the results in the file fidexGloStats.txt. With --global_rules_outfile we can generate the statistics on rules which will modify the rules file. If you want to keep the first ruleSet unchanged, you should give another name. Finally, with --positive_class_index we can specify the positive class in order to get a ROC curve and some other statistics on false and true positives/negatives. We will design the malignant class as positive. So here is the command :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "375ef5c0-aca8-4e1a-a842-ed229d23e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters list:\n",
      " - test_data_file                                                          data/BCWDataset/testData.txt\n",
      " - test_pred_file                                                          data/BCWDataset/predTest.out\n",
      " - test_class_file                                                        data/BCWDataset/testClass.txt\n",
      " - global_rules_outfile                                                 data/BCWDataset/globalRules.rls\n",
      " - global_rules_file                                                    data/BCWDataset/globalRules.rls\n",
      " - root_folder                                                                          data/BCWDataset\n",
      " - attributes_file                                                       data/BCWDataset/attributes.txt\n",
      " - stats_file                                                         data/BCWDataset/fidexGloStats.txt\n",
      " - nb_attributes                                                                                     30\n",
      " - nb_classes                                                                                         2\n",
      " - positive_class_index                                                                               0\n",
      "End of Parameters list.\n",
      "\n",
      "Importing files...\n",
      "\n",
      "Data imported.\n",
      "\n",
      "Compute statistics...\n",
      "\n",
      "Global statistics of the rule set : \n",
      "Number of rules : 21, mean sample covering number per rule : 76.190476, mean number of antecedents per rule : 1.761905\n",
      "\n",
      "Statistics with a test set of 143 samples :\n",
      "\n",
      "No decision threshold is used.\n",
      "Positive index class used : 0\n",
      "The global rule fidelity rate is : 0.979021\n",
      "The global rule accuracy is : 0.972028\n",
      "The explainability rate (when we can find one or more rules, either correct ones or activated ones which all agree on the same class) is : 0.958042\n",
      "The default rule rate (when we can't find any rule activated for a sample) is : 0.041958\n",
      "The mean number of correct(fidel) activated rules per sample is : 3.195804\n",
      "The mean number of wrong(not fidel) activated rules per sample is : 0.083916\n",
      "The model test accuracy is : 0.979021\n",
      "The model test accuracy when rules and model agree is : 0.985714\n",
      "The model test accuracy when activated rules and model agree is : 0.992537\n",
      "\n",
      "With positive class malignant :\n",
      "\n",
      "Computation with model decision :\n",
      "The number of true positive test samples is : 35\n",
      "The number of false positive test samples is : 3\n",
      "The number of true negative test samples is : 105\n",
      "The number of false negative test samples is : 0\n",
      "The false positive rate is : 0.027778\n",
      "The false negative rate is : 0.000000\n",
      "The precision is : 0.921053\n",
      "The recall is : 1.000000\n",
      "\n",
      "Computation with rules decision :\n",
      "The number of true positive test samples is : 35\n",
      "The number of false positive test samples is : 4\n",
      "The number of true negative test samples is : 104\n",
      "The number of false negative test samples is : 0\n",
      "The false positive rate is : 0.037037\n",
      "The false negative rate is : 0.000000\n",
      "The precision is : 0.897436\n",
      "The recall is : 1.000000\n",
      "\n",
      "Full execution time = 0.049806 sec\n"
     ]
    }
   ],
   "source": [
    "#res = fidex.fidexGloStats(\"--test_data_file testData.txt --test_pred_file predTest.out --test_class_file testClass.txt --attributes_file attributes.txt --global_rules_file globalRules.rls --global_rules_outfile globalRules.rls --nb_attributes 30 --nb_classes 2 --stats_file fidexGloStats.txt --positive_class_index 0 --root_folder data/BCWDataset\")\n",
    "res = fidexGloStats(\"--json_config_file data/BCWTemplates/config_fidexGloStats.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c523c382-b2ed-413e-84e7-c0d39c6d1bf5",
   "metadata": {},
   "source": [
    "Here you can see the global statistics on the test set. As the previous calculations have randomness, each execution is a bit different. So here we will discuss the same case as before, so you may have slightly different results. In our case, we had about 98% fidelity, which is good, and a rule accuracy(96.5%) about 1.5% lower than the model accuracy(98%). So the rules seem to classify a bit worse. The explainability rate is the percentage of samples for which we can find a rule in the rules set. For the others, we need to execute Fidex (this is the default rule rate). In our case, we had more than a 97% explainability rate, so only in 3% of cases do we need to compute Fidex. Each rule can activate many rules. Here on average, a sample activates 5 correct rules and 0.05 wrong rules. A wrong rule is a rule with which the model doesn't agree. For example, if the rule says malign and the model says benign. Something interesting is the model test accuracy when rules and model agree. You can see that, generally, the accuracy increases if we consider samples where rules and model agree, and increases even more if we take only the activated rules (when there are no activated rules, we choose the model prediction). That means that the rules confirm well the model decision, but when no rule is found, the model decision may as well be wrong. <br>\n",
    "\n",
    "Finally, we have the statistics on the positive/negative decisions of the model and of the rules. For the model's decisions, we had 1% false positives and 7% false negatives. That means that when the model says that it's malign, it has a 99% chance of being malign. But when it says that it's benign, there is a 7% risk that it is malign in reality. So the precision is 97% and the recall is 93%.\n",
    "\n",
    "In the case of the rules decisions, the rules decision is the same as the model if we find a correct rule. If there is no rule, we can launch Fidex, so it's also the same decision as the model because it will create a fidel rule. The only scenario when the decision changes from the model's is when some rules are activated, all rules decide the same class which is not the class chosen by the model. The results are really close with 95% of precision and 93% of recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5400680-5058-4f75-97f1-552b1cead6d8",
   "metadata": {},
   "source": [
    "In globalRules file, you can now see the statistics of rules on the test set. Here is the same rule as before that I have now:<br>\n",
    "\n",
    "Rule 1: MEAN_CONCAVE_POINTS<0.041282 WORST_CONCAVE_POINTS<0.150829 -> BENIGN <br>\n",
    "   Train Covering size : 217 --- Test Covering size : 54 <br>\n",
    "   Train Fidelity : 1 --- Test Fidelity : 0.981481 <br>\n",
    "   Train Accuracy : 0.990783 --- Test Accuracy : 1 <br>\n",
    "   Train Confidence : 0.984811 --- Test Confidence : 0.979816 <br>\n",
    "\n",
    "You can see that the rule no longer always agrees with the model, only in 98.15% of cases. However, the rule accuracy has increased with the test. That means that the rule is very good in reality, with 100% of correct classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c457745a-6030-486e-95d4-33040fe430ea",
   "metadata": {},
   "source": [
    "We can get a ROC curve obtained with the test set. Here are the parameters of computeRocCurve which is located in the trainings package :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "338b8a62-832e-45f7-8540-9ec4a7928447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: \n",
      "--test_class_file <str> --test_pred_file <str> --positive_class_index <int [0,nb_classes-1]> --nb_classes <int [1,inf[> [-h, --help] [--json_config_file <str>] [--root_folder <str>] [--stats_file <str>] [--show_params <bool>] [--output_roc <str>] [--estimator <str>]\n",
      "\n",
      "This is a parser for computeRocCurve\n",
      "\n",
      "\n",
      "Parameters:\n",
      "\n",
      "  ---------------------------------------------------------------------\n",
      "\n",
      "  Warning! The files are located with respect to the root folder dimlpfidex.\n",
      "\n",
      "  The arguments can be specified in the command or in a json configuration file with --json_config_file your_config_file.json.\n",
      "\n",
      "  ----------------------------\n",
      "\n",
      "  Required parameters:\n",
      "\n",
      "  --test_class_file <str>                        Path to the file containing the test true classes of the dataset\n",
      "  --test_pred_file <str>                         Test prediction file\n",
      "  --positive_class_index <int [0,nb_classes-1]>  Index of the positive class, index starts at 0\n",
      "  --nb_classes <int [1,inf[>                     Number of classes in the dataset\n",
      "\n",
      "  ----------------------------\n",
      "\n",
      "  Optional parameters:\n",
      "\n",
      "  -h --help                                      show this help message and exit\n",
      "  --json_config_file <str>                       Path to the JSON file that configures all parameters. If used, this must be the sole argument and\n",
      "                                                 must specify the file's relative path\n",
      "  --root_folder <str>                            Path to the folder, based on main default folder dimlpfidex, containing all used files and where\n",
      "                                                 generated files will be saved. If a file name is specified with another option, its path will be\n",
      "                                                 relative to this root folder> (default: \"\")\n",
      "  --stats_file <str>                             Path to the file where the AUC score will be added, it can be the training stats file\n",
      "  --show_params <bool>                           Whether to show the parameters (default: True)\n",
      "\n",
      "  ----------------------------\n",
      "\n",
      "  ROC parameters (optional):\n",
      "\n",
      "  --output_roc <str>                             Path to the file where the output ROC curve will be saved (default: roc_curve.png)\n",
      "  --estimator <str>                              Name of the estimator\n",
      "\n",
      "  ----------------------------\n",
      "\n",
      "Execution example :\n",
      "\n",
      "computeRocCurve('--test_class_file dataclass2Test.txt --test_pred_file predTest.out --positive_class_index 1 --output_roc roc_curve.png --stats_file stats.txt --root_folder dimlp/datafiles --nb_classes 2')\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = computeRocCurve(\"--help\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec3ff5-a6db-4478-9d36-87f2ddbc3247",
   "metadata": {},
   "source": [
    "We execute computeRocCurve :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c817149-bae3-4ce2-8bc8-074a27747705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#res = computeRocCurve('--test_class_file testClass.txt --test_pred_file predTest.out --positive_class_index 0 --output_roc outRoc.png --stats_file stats.txt --root_folder data/BCWDataset --nb_classes 2 --show_params false')\n",
    "res = computeRocCurve(\"--json_config_file data/BCWTemplates/config_roc.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a27b096-fbfd-4c67-a303-58a5b404169b",
   "metadata": {},
   "source": [
    "The AUC score is added to the stats file and the ROC curve is saved in the outRoc.png file. We get an AUC of about 99%. <br>\n",
    "We can visualize the AUC curve :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd579681-492d-4900-84ae-69aeef690a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(filename=\"data/BCWDataset/outRoc.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6709efc3-5ac9-41d8-927e-1c5fedddef82",
   "metadata": {},
   "source": [
    "Finally, it's possible to launch fidexGlo on a test set. It will, for each test sample, search in the global ruleset for all the rules that verify the sample's properties and match the decision of the model. If no rule verifies the properties, it calls Fidex on the test sample. In the other case, it gives all the correct and wrong activated rules.<br>\n",
    "\n",
    "FidexGlo is located in the fidex module. Here are the possible parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24ccff77-b9a8-4c97-b3f7-2f8756935bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Warning! The files are located with respect to the root folder dimlpfidex.\n",
      "The arguments can be specified in the command or in a JSON configuration file with --json_config_file your_config_file.json.\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Required parameters:\n",
      "\n",
      "--test_data_file <str>        Path to the file containing test sample(s) data, prediction (if no --test_pred_file) and true classes if launching with fidex (--with_fidex and if no --test_class_file)\n",
      "--test_pred_file <str>        Path to the file containing predictions on the test portion of the dataset. If given, --test_data_file needs to have only the test data\n",
      "--global_rules_file <str>     Path to the file containing the global rules obtained with fidexGloRules algorithm.\n",
      "--nb_attributes <int [1,inf[> Number of attributes in the dataset\n",
      "--nb_classes <int [2,inf[>    Number of classes in the dataset\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Optional parameters: \n",
      "\n",
      "-h --help                     Show this help message and exit\n",
      "--json_config_file <str>      Path to the JSON file that configures all parameters. If used, this must be the sole argument and must specify the file's relative path\n",
      "--root_folder <str>           Path to the folder, based on main default folder dimlpfidex, containing all used files and where generated files will be saved. If a file name is specified with another option, its path will be relative to this root folder\n",
      "--explanation_file <str>      Path to the file where explanation(s) will be stored\n",
      "--attributes_file <str>       Path to the file containing the labels of attributes and classes. Mandatory if rules file contains attribute names; if not, do not add it\n",
      "--console_file <str>          Path to the file where the terminal output will be redirected. If not specified, all output will be shown on your terminal\n",
      "--with_minimal_version <bool> Whether to use the minimal version, which only gets correct activated rules and if with_fidex, launches Fidex when no such rule is found (default: False)\n",
      "--with_fidex <bool>           Whether to use the Fidex algorithm if no rule is found in global rules (default: False)\n",
      "\n",
      "----------------------------\n",
      "\n",
      "If using fidex :\n",
      "\n",
      "Required :\n",
      "\n",
      "--train_data_file <str>       Path to the file containing the train portion of the dataset\n",
      "--train_class_file <str>      Path to the file containing the train true classes of the dataset, not mandatory if classes are specified in train data file\n",
      "--train_pred_file <str>       Path to the file containing predictions on the train portion of the dataset\n",
      "--weights_file <str>          Path to the file containing the trained weights of the model (not mandatory if a rules file is given with --rules_file)\n",
      "--rules_file <str>            Path to the file containing the trained rules to be converted to hyperlocus (not mandatory if a weights file is given with --weights_file)\n",
      "\n",
      "Optional :\n",
      "\n",
      "--test_class_file <str>       Path to the file containing the test true classes of the dataset. Classes can be specified in test data file\n",
      "--max_iterations <int [1,inf[>\n",
      "                              Maximum number of iterations, also the maximum possible number of antecedents in a rule, it should be 25 when working with images (default: 10)\n",
      "--min_covering <int [1,inf[>  Minimum number of samples covered by the generated rules (default: 2)\n",
      "--covering_strategy <bool>    Whether to use the covering strategy : if no rule is found with min_covering, find best rule with best covering using dichotomic search. Decreases min_fidelity if needed (default: True)\n",
      "--max_failed_attempts <int [0,inf[>\n",
      "                              Maximum number of failed attempts to find a Fidex rule when the covering is 1 and the covering strategy is used (default: 30)\n",
      "--min_fidelity <float [0,1]>  Minimal rule fidelity accepted when generating a rule (default: 1.0)\n",
      "--lowest_min_fidelity <float [0,1]>\n",
      "                              Minimal min_fidelity to which we agree to go down during the covering_strategy (default: 0.75)\n",
      "--nb_fidex_rules <int [1,inf[>\n",
      "                              Number of Fidex rules to compute per sample when launching the Fidex algorithm (default: 1)\n",
      "--dropout_dim <float [0,1]>   Probability of dropping a dimension during rule extraction (default: 0.0)\n",
      "--dropout_hyp <float [0,1]>   Probability of dropping a hyperplane during rule extraction (default: 0.0)\n",
      "--nb_quant_levels <int [3,inf[>\n",
      "                              Number of stairs in the staircase activation function (default: 50)\n",
      "--normalization_file <str>    Path to the file containing the mean and standard deviation of some attributes. Used to denormalize the rules if specified\n",
      "--mus <list<float ]-inf,inf[>>\n",
      "                              Mean or median of each attribute index to be denormalized in the rules\n",
      "--sigmas <list<float ]-inf,inf[>>\n",
      "                              Standard deviation of each attribute index to be denormalized in the rules\n",
      "--normalization_indices <list<int [0,nb_attributes-1]>>\n",
      "                              Attribute indices to be denormalized in the rules, only used when no normalization_file is given, index starts at 0 (default: [0,...,nb_attributes-1])\n",
      "--seed <int [0,inf[>          Seed for random number generation, 0=random. Anything else than 0 is an arbitrary seed that can be reused to obtain the same randomly generated sequence and therefore getting same results (default: 0)\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Execution example :\n",
      "\n",
      "fidex.fidexGlo(\"--test_data_file datanormTest.txt --test_pred_file predTest.out --global_rules_file globalRules.rls --nb_attributes 16 --nb_classes 2 --explanation_file explanation.txt --root_folder dimlp/datafiles --with_fidex true --train_data_file datanormTrain.txt --train_pred_file predTrain.out --train_class_file dataclass2Train.txt --test_class_file dataclass2Test.txt --weights_file weights.wts\")\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = fidexGlo(\"--help\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c845e1fe-9226-4aa7-874d-eb582695afda",
   "metadata": {},
   "source": [
    "As we want to use Fidex on test sample not covered by the global ruleset, we need to specify the Fidex parameters. We don't forget to add the normalization file and launch it with :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ec3cd7f-502b-49c4-b1ce-28f219b8d5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanations generated\n"
     ]
    }
   ],
   "source": [
    "#res = fidex.fidexGlo(\"--test_data_file testData.txt --test_pred_file predTest.out --global_rules_file globalRules.rls --nb_attributes 30 --nb_classes 2 --attributes_file attributes.txt --explanation_file explanations.txt --console_file fidexGloResults.txt --root_folder data/BCWDataset --with_fidex true --train_data_file trainData.txt --train_pred_file predTrain.out --train_class_file trainClass.txt --test_class_file testClass.txt --weights_file weights.wts --normalization_file normalization_stats.txt\")\n",
    "res = fidexGlo(\"--json_config_file data/BCWTemplates/config_fidexGlo.json\")\n",
    "\n",
    "if res == 0:\n",
    "    print(\"Explanations generated\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccf58f4-b70b-4586-ad3c-345d9a5d0823",
   "metadata": {},
   "source": [
    "The explanations for each test sample can then be found in the file explanations.txt. We do not find a global rule for around 2.6% of the samples, for which we calculate a local rule using Fidex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7f3208-c538-40eb-8c76-31e75d687198",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Throughout this notebook, we've learned how to use DimlpBT to train a dataset and get some explaining rules, and we got into the different Fidex algorithms to get some local and global explanations, and some statistics on the model's decisions. We used the example of the breast cancer dataset from Wisconsin and we saw that the algorithms perform very well. It gives some good explanations of the model's decision and does it very fast, especially if a fidexGlo ruleset has been computed beforehand. You can try with any tabular dataset you want, you just have to remember to transform the data in a good format. <be>\n",
    "\n",
    "We didn't speak about every single parameter, you can try to change every hyperparameter you want to see how it goes. An interesting parameter that we didn't consider is the parameter for decision threshold (--decision_threshold) in Fidex algorithms. It allows us to change predictions with respect to a specific threshold on the positive class. If the model gives a score prediction, for the positive class, greater than this threshold, the model prediction is considered to be the positive class, even if another class obtains a higher score. If the recall is not good enough, it's possible to improve it this way. Another parameter is the seed. It allows you to remove the randomness of the algorithms and get the same result in each execution for the same parameters and data.<br>\n",
    "\n",
    "We considered only a simple tabular dataset. It's also possible to use it for an image classification problem. To see how we can train a dataset with convolutions, we recommend exploring the [`Mnist notebook`](mnist.ipynb) for a hands-on experience. There are also other training methods, like an MLP or decision trees. The [`Obesity CVD`](obesityCvdRisk.ipynb) notebook goes through these and shows what to do when we have categorical attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605dcb45-4862-4268-bef2-068cf96de3d9",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "HES-XPLAIN: [website](https://hes-xplain.github.io/), [Github page](https://github.com/HES-XPLAIN)\n",
    "\n",
    "Dataset: [source](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data)\n",
    "\n",
    "Dimlpfidex: [Github repository](https://github.com/HES-XPLAIN/dimlpfidex), [documentation](https://hes-xplain.github.io/documentation/overview/)\n",
    "\n",
    "Algorithms: [randomForest](https://hes-xplain.github.io/documentation/dimlpfidex/training-methods/randforeststrn/), [Fidex](https://hes-xplain.github.io/documentation/dimlpfidex/fidex/fidex/), [FidexGloRules](https://hes-xplain.github.io/documentation/dimlpfidex/fidex/fidexglorules), [FidexGloStats](https://hes-xplain.github.io/documentation/dimlpfidex/fidex/fidexglostats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
