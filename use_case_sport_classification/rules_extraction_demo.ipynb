{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf44953f-b4e9-401d-9851-48441fbc908f",
   "metadata": {},
   "source": [
    "# Exploring Rules Extraction for Sport Image Classification\n",
    "\n",
    "**Introduction:**\n",
    "\n",
    "Welcome to HES-Xplain, our interactive platform designed to facilitate explainable artificial intelligence (XAI) techniques. In this `demo`, we dive into sport image classification and showcase the power of Rules Extraction as an interpretability tool.\n",
    "\n",
    "Rules Extraction allow us to reduce the complexity of the CNN (fully-connected layers) to a small set of relevant rules, without a great loss in accuracy. Furthermore, these rules can be interpreted by looking at how they split the input data. Such a combined analysis helps us to better understand the global behavior of the network.\n",
    "\n",
    "The library implementation and this demo mostly follow the paper [Improving neural network interpretability via rule extraction](https://arodes.hes-so.ch/record/4647?ln=fr). And aim at replacing the fully-connected layers of a Convolutional Neural Network (CNN) with a small set of rules, allowing for better interpretation of its decisions while preserving accuracy. It create Ensemble Rule based binary classifier that is able to discriminate between a target class versus others. \n",
    "\n",
    "With our [rules-extraction library](https://github.com/HES-XPLAIN/rules-extraction), you can effortlessly extract rules from a CNN model and seamlessly utilize this ensemble of rules in a manner compatible with the scikit-learn API. You can also create plot that illustrate the data and rules. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e5fee-c65c-4c94-867e-8f630a10e573",
   "metadata": {},
   "source": [
    "---\n",
    "## Google Colab Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261f98f3-a8fa-41f5-a2f9-8f61b5c612d1",
   "metadata": {},
   "source": [
    "This section prepares the notebook for use with Google Colaboratory. If applicable, change the following variable to True:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2e2da3-7fbb-44c0-8d41-df6ab8a4f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab compatibility\n",
    "use_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c92821d-b73d-458b-ac08-3d85eb98c80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_colab:\n",
    "    # ensure the directory is empty\n",
    "    !rm -rf * .config\n",
    "\n",
    "    !# install codebase from GitHub\n",
    "    !git clone --no-checkout https://github.com/HES-XPLAIN/notebooks.git --depth=1 .\n",
    "    !git config core.sparseCheckout true\n",
    "    !git sparse-checkout set --cone\n",
    "    !git sparse-checkout add use_case_sport_classification\n",
    "    !git sparse-checkout reapply\n",
    "    !git checkout main\n",
    "\n",
    "    # adjust folder structure\n",
    "    !mv use_case_sport_classification/* .\n",
    "    !rm -rf use_case_sport_classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b08be9-a783-475e-a558-99d95c48bfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_colab:\n",
    "    # Install dependencies\n",
    "    !pip install rules_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cae22e0-1ec5-4fe6-9f19-eb95794d7710",
   "metadata": {},
   "source": [
    "When asked to, restart the session by clicking on the \"Restart Session\" button, then continue and execute the next cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a526c71-46b0-41cf-b588-998db5462213",
   "metadata": {},
   "source": [
    "## Workspace Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f865ae-2790-4059-a227-58e21979083f",
   "metadata": {},
   "source": [
    "This section download the required code and models from our GitHub and huggingface.co repositories.\n",
    "\n",
    "Download the dataset and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91ea5d3-3ffa-4704-aa0d-e8494b683c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and extract dataset\n",
    "import zipfile\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "REPO_ID = \"HES-XPLAIN/SportsImageClassification\"\n",
    "FILENAME = \"SportsImageClassification.zip\"\n",
    "dataset_file_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=\"dataset\")\n",
    "extract_path = 'data'\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(dataset_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(f\"Dataset successfully extracted to: {extract_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763e83b3-c82b-4977-b41c-3e460b5a3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models_weight\n",
    "!curl -OL https://huggingface.co/HES-XPLAIN/sport_classification/resolve/main/VGGSportsImageClassification.pth -o VGGSportsImageClassification.pth\n",
    "!mv *.pth models_weight/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97733ac6-93e8-45af-8d27-76bb5fdcb6a6",
   "metadata": {},
   "source": [
    "## Dataset and Problem Statement\n",
    "\n",
    "The sport image classification dataset used in this use case consists of approximately 15,000 images, covering 100 different sports categories. This dataset, available on [Kaggle](https://www.kaggle.com/datasets/gpiosenka/sports-classification), contains a total of 13,493 train images, 500 test images, and 500 validation images. The images are in JPG format and have dimensions of 224x224 pixels with 3 color channels.\n",
    "\n",
    "**Download dataset**: To make a propoer use of this demo, the dataset will be automatically downloaded and extracted in the data folder. You can download it using the next cell or from our [Hugging Face repository](https://huggingface.co/datasets/HES-XPLAIN/SportsImageClassification).\n",
    "\n",
    "**Problem Statement**: Our objective is to build a robust image classifier capable of accurately classifying sport images among the 100 predefined sports categories. The nature of the dataset, comprising images of various sports, makes it particularly suitable for introducing and visualizing XAI techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd165687-e574-49d3-9bc3-5da94d2fc05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global imports \n",
    "import sys\n",
    "import os\n",
    "\n",
    "from rules_extraction.rules import Rule, EnsembleRule, RuleRanker\n",
    "from rules_extraction.plot import plot_accuracy, plot_frontier\n",
    "from rules_extraction.utils import *\n",
    "\n",
    "from scripts.models import FineTunedVGG\n",
    "from scripts.custom_dataset import CustomDataset\n",
    "from scripts.helpers import *\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c60fe9-a121-4995-be06-58774a7d4930",
   "metadata": {},
   "source": [
    "## Loading a CNN model\n",
    "\n",
    "**Model Weights**: We provide weights for a fine-tuned VGG on the SportsImageDataset. You can download it using the next cell or from our [Hugging Face repository](https://huggingface.co/HES-XPLAIN). While using Github Codespace you can have direct access to the weight in the folder `./models_weight`.\n",
    "\n",
    "**Loading Weights** : By utilizing the functions and classes defined in our helpers file, you can initialize a VGG model and load the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eb60ea-8af4-4970-a2dd-480d1df1e4aa",
   "metadata": {},
   "source": [
    "---\n",
    "Utilize GPU and CUDA for faster processing speed and load the weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21415bd4-47f3-446c-8627-c2e8de00be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA (GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA (GPU) not available. Using CPU.\")\n",
    "\n",
    "# load trained model VGG\n",
    "model = FineTunedVGG()\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Specify the map_location argument when loading the model\n",
    "load_path = \"models_weight/VGGSportsImageClassification.pth\"\n",
    "checkpoint = torch.load(load_path, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "# get loaders  using function define in scripts for this notebook\n",
    "train_loader, valid_loader, test_loader = get_dataloaders()\n",
    "\n",
    "# get a dictionary that links class names to integers, making it easier for us as we proceed in the notebook\n",
    "with open('./data/idx_to_names.json', 'r') as file:\n",
    "    idx_to_names = json.load(file)\n",
    "names_to_idx = {v: k for k, v in idx_to_names.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0851cc4a-6bbd-4ca8-919b-a5803809652a",
   "metadata": {},
   "source": [
    "## Rules extraction\n",
    "\n",
    "From this point, we'll utilize class and functions from the rules-extraction python library, accessible on our [HES-XPLAIN platform](https://github.com/HES-XPLAIN) and the [pypi index](https://pypi.org/project/rules-extraction).\n",
    "\r\n",
    "**Filter dataset**: Let's create a filtered version of the data. To do this we will initialize a dataloader using only the subset of data that are correctly classified by the CNN model. This is not mandatory but it allow us to establish the CNN's performane as the ground truth. This step enable us to assess the extent of deviation between our simplification and the standard model. We can use the `rules_extraction.utils.filter_dataset` function to get the list of well classified data index. From this we can initialize a new loader.\n",
    "\n",
    "**Compute image representation**: The rules are derived from an image representation generated by the model, specifically the final feature map of the CNN. This feature map is averaged to yield a vector, providing a condensed representation of the image. The functions `rules_extraction.utils.compute_avg_features` implement this logic and returns a dataframe.\n",
    "\n",
    "**Create a target class dataset**: The Ensemble Rules model will be able to perform binary classification, thus we need to parse the dataset to create a binary dataset containing N datapoints of target class and N of random other class. This allow to have a balance dataset.\n",
    "\n",
    "**Extract rules**: Once we got the image representation we can extract rules from a RandomForestClassifier model using the `rules_extraction.utils.extract_all_rules` function. You have the flexibility to pass parameters to customize your Random Forest model as needed. We recommend using `max_depth=2` to ensure the rules remain understandable.\n",
    "\n",
    "**Rank Rules**: To reduce complexity we can rank rules and we will later use only some of the top ranked rules to. This method use a simple percpetron and use weight to classify rules importance. The logic is encapsulated in the `rules_extraction.rules.RuleRanker.rank_rules` method.l."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1f34f3-fae3-4489-9f9b-7d5f8f78b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create filtered loaders\n",
    "\n",
    "model = model\n",
    "loader = test_loader\n",
    "device = device \n",
    "\n",
    "# test set\n",
    "correct_test_idx = filter_dataset(model=model, loader=loader, device=device)\n",
    "test_filtered_dataset = Subset(test_loader.dataset, correct_test_idx)\n",
    "test_filtered_dataloader = DataLoader(dataset=test_filtered_dataset, batch_size=test_loader.batch_size, shuffle=False)\n",
    "\n",
    "loader = train_loader\n",
    "#train set\n",
    "correct_train_idx = filter_dataset(model=model, loader=loader, device=device)\n",
    "train_filtered_dataset = Subset(train_loader.dataset, correct_train_idx)\n",
    "train_filtered_dataloader = DataLoader(dataset=train_filtered_dataset, batch_size=train_loader.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749ac5ee-ca19-4e04-b171-f014b52d6130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute avg features map \n",
    "\n",
    "test_features = compute_avg_features(model, test_filtered_dataloader, idx_to_names, device)\n",
    "train_features = compute_avg_features(model, train_filtered_dataloader, idx_to_names, device)\n",
    "\n",
    "test_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b01ab8d-3c72-453a-8992-778685ceef93",
   "metadata": {},
   "source": [
    "As you can see, our dataframe now represents each image with a vector of 512 floats. These vectors are obtained by averaging the last feature map of VGG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddce838f-d85b-4716-a42c-0d0658468cbc",
   "metadata": {},
   "source": [
    "---\n",
    "As mentioned earlier, our current task involves binary classification, specifically targeting the classification of one specific category against the rest. To achieve this, we must dynamically parse our dataframe and generate a binary dataset on the fly. \n",
    "\n",
    "For this, we select a target class of interest and employ the function rules_extraction.utils.make_target_df. This function returns a dataframe containing all the data for the target class, while also sampling from other classes to create a balanced dataset.\n",
    "\n",
    "Let's investigate the 'air hockey' class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d90ca9b-42fe-4057-b4fa-d1475eb173bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = make_target_df(test_features, 'air hockey')\n",
    "df_train = make_target_df(train_features, 'air hockey')\n",
    "X_train, y_train = df_train.iloc[:, :-3], df_train.iloc[:, -1]\n",
    "X_test, y_test = df_test.iloc[:, :-3], df_test.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972bf145-56b7-460d-854d-dd69bf52e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all rules from a RandomForestClassifier\n",
    "all_rules = extract_all_rules(X_train, y_train, n_estimators=200, max_depth=2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c3b8c7-e7a1-4126-91eb-604ea3ea9cb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rank rules extract only the top 30 \n",
    "top_30_rules = RuleRanker(all_rules, X_train, y_train).rank_rules(N=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baf0f2e-0dbf-4d33-a31d-d6500da7ffe8",
   "metadata": {},
   "source": [
    "## Create rule based model \n",
    "\n",
    "**Ensemble Rule Modeling**: \n",
    "\n",
    "Once the rules are ranked, we can make use of our rule classifier API. To begin, we initialize individual rules using our simple rule class and then select the top N rules to initialize the EnsembleRule classifier.\r\n",
    "\r\n",
    "Both the Rule and EnsembleRule are designed to be compatible with the scikit-learn API. They implement the .fit, .predict, and .score methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9296961-df7e-4553-868c-fe6f6f023d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract conditions and labels from the list and initialize a list of Rule\n",
    "conditions_list, labels_list = zip(*top_30_rules)\n",
    "rules = [Rule(conditions, label) for conditions, label in zip(conditions_list, labels_list)]\n",
    "\n",
    "# Initialize ensemble rule\n",
    "ensemble_rule = EnsembleRule(rules)\n",
    "\n",
    "# Predict with ensemble rule\n",
    "data_point = df_test.iloc[0]\n",
    "ensemble_prediction = ensemble_rule.predict(data_point)\n",
    "print(f\"Ensemble Rule Prediction: {ensemble_prediction[0]} the correct prediction is {data_point['binary_label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9006cc8c-a32c-4170-b605-be25d13699a9",
   "metadata": {},
   "source": [
    "## Rules Extraction results and plots.\n",
    "\n",
    "**Prediction and score**: We can use ensemble.predict or rule.predict to make prediction on one or multiple data points and use the ensemble.score or rule.score method to compute the accuracy. \n",
    "\n",
    "**Plots**: The `rules_extraction.plot` API enable two plots. \n",
    "\n",
    "1. Plot accuracy: where you can compute and plot the accuracy on a dataset versus the number of rules used. This show how adding rules improve the model. Usually we can see that from two to three rules our EnsembleRule classifier is able to achieve 80-90% accuracy.\n",
    "\n",
    "2. Plot frontier: this plot show a rule for one class with example images and plot the rule's frontier. Images are embedded according to their average filter activation. The most characteristic images of the class are grouped together as they have similar filter activations. We can also get the intuition for visual patterns that are present in one class but absent in all other classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46677917-d59d-4299-8181-46c11dffb0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = ensemble_rule.predict(X_test)\n",
    "print(f\"Ensemble Rule predictions on test set are: {ensemble_predictions}. And the correct predictions are {y_test.tolist()}\")\n",
    "print(f\"Accuracy on test set: {ensemble_rule.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e1035d-d3da-402e-8e61-49d711e0367d",
   "metadata": {},
   "source": [
    "---\n",
    "Let's do some plots now: the `rules_extraction.plot` API enable two plots. \n",
    "\n",
    "1. Plot accuracy: where you can compute and plot the accuracy on a dataset versus the number of rules used. This show how adding rules improve the model. Usually we can see that from two to three rules our EnsembleRule classifier is able to achieve 80-90% accuracy.\n",
    "\n",
    "2. Plot frontier: this plot show a rule for one class with example images and plot the rule's frontier. Images are embedded \n",
    "according to their average filter activation The most characteristic images of the class are grouped together as they have \n",
    "similar filter activations. We can also get th  intuition for visual patterns tha \r\n",
    "are present in one class but absent in all other classes.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222d20c0-99be-45c6-9c34-a1e77e40dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(rules, df_train, class_name=None, n=9, save_path=None)\n",
    "plot_accuracy(rules, df_test, class_name=None, n=9, save_path=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04561067-b97d-4ec5-856f-7dffdbe03caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frontier(train_features, rule=rules[1], target_class='air hockey', model=model, alpha=0.55, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8e59b1-d6b1-4c80-ab0c-b1f973885f46",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "Throughout this notebook, we demonstrate the application of our rules-extraction library for extracting rules from a convolutional model. Additionally, we illustrate how to use these rules to make predictions employing a simpler and more interpretable model.\r\n",
    "\r\n",
    "Utilizing an ensemble of rule-based models empowers users to have control over their predictions and offers potential explanations. Although average features may not be directly understandable by humans, they contribute to reducing the complexity of a CNN. Further exploration can be undertaken to delve into the significant features, including those used by the top rule.\r\n",
    "\r\n",
    "Furthermore, by employing the frontier plot, we aim to gain intuition into visual patterns that are present in one class but absent in all other classes. This approach contributes to enhancing the explainability of the CNN model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8ec7ad-022b-471b-8a14-c2e5f3051ec1",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [Rule Extraction paper](https://arodes.hes-so.ch/record/4647?ln=fr)\n",
    "2. [Scikit-learn](https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95871d8-c46f-4b8c-9703-43ae9f43ef0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
